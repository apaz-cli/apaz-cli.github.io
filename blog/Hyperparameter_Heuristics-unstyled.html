<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Hyperparameter Heuristics</title>
</head>
<body>
<h1 id="hyperparameter-heuristics">Hyperparameter Heuristics</h1>
<p><br></p>
<div style="text-align: center;">
<p><img src="images/107214767_p0.png" /></p>
</div>
<p><br> <br></p>
<p>    It would be cool if there was a guide on heuristics for scaling
training experiments.</p>
<p>    The information is sort of trapped inside the heads and notebooks
of individual researchers at the moment. It's true that it doesn't make
sense for companies other than Meta/OpenAI/Anthropic to do pretraining
runs. But I feel like the information for running the experiments for
doing so should be more accessible. Maybe such a guide exists somewhere
and I'm an idiot for not finding it. But this knowledge is certainly not
as widespread as projects like litgpt (which I maintain) and torchtune,
which are the projects that researchers typically use for running
training and scaling experiments.</p>
<p>    In particular I'm interested in using hparam sweeps on smaller
models to figure out the optimal hparams for larger models. I know that
the choice of hyperparameters that's optimal for training a small model
doesn't necessarily generalize to training a large model. But some of
them do, and there are heuristics that people use to make guesses. The
learning rate, for example, is highly dependent on the batch size and
other aspects of numerical stability that change in a predictable
fashion as you add parameters. What formula can we use to make a good
guess for the larger model? These things are not very well known. There
are certain papers (like Chinchilla for example) that contain a lot of
these useful heuristics. But what about the people who have never heard
of Chinchilla or the twenty other papers the heuristics are scattered
across?</p>
<p>    Most of the time, people just copy over the hparams from training
runs they knew were successful. I think we can do better. People need to
be able to discover their own hparams for their experiments, and they
need to be able to verify the work of others.</p>
<p>    I'm reminded of chemists that buy chemicals and then re-purify
them themselves, because they don't trust the chemical companies.
Although it's probably fine, what if it isn't? Your experiment is
screwed, and you might not even know it. The same is very true with
hparams. Always purify your chems and validate your hparams. Because
sometimes, the hparams that get published are just wrong. I've had many
conversations with people along the lines of "The learning rate in that
paper was way too high, and there are better normalizations and
initializations."</p>
<p>    A lot of researchers also just don't do hparam sweeps, because
they're too expensive, because they can't run small experiments and
generalize the results to larger ones. Because they don't know the
heuristics either. The result is that we just don't know what recipes
are good vs bad. I think that increasing the rigor here could benefit
the field substantially, and also make the field more accessible.</p>
</body>
</html>
