<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" href="index.css">
    <link rel="stylesheet" href="sidebar.css">
    <link rel="icon" type="image/ico"
        href="https://raw.githubusercontent.com/apaz-cli/apaz-cli.github.io/master/favicon.ico">
    <title>Aaron Pazdera's Portfolio</title>
    <style>
        .model_graph {
            background-color: blanchedalmond;
            border-radius: 8px;
            width: 65%;
            height: auto;
        }
    </style>
</head>


<body>
    <div class="sidebar">
        <h3 id="Contents">&nbsp;Contents:</h3>
        <hr>
        <a href="#Introduction">Introduction</a>
        <a href="#Economic-Modeling">Economic Modeling Research</a>
        <a href="#Perceptual-Image-Hashing">Perceptual Image Hashing</a>
        <a href="#MLPMixer">MLP-Mixer</a>
        <a href="#Deep-Learning-Hash">Deep Learning Hash</a>
        <a href="#Safebooru-Scraper">Safebooru Scraper</a>
        <a href="#VPTree">Vantage Point Tree</a>
        <a href="#CProj">Other C Projects</a>
        <a href="#Brainfuck-Compiler-Interpreter">BF Compiler/Interpreter</a>
        <a href="#RootwallaBot">RootwallaBot</a>
        <a href="#MonikaBot">MonikaBot</a>

        <div id="bottom">
            <h4 id="Contact">&nbsp;Contact:</h4>
            <hr>
            <a href="mailto:aarpazdera@gmail.com">aarpazdera@gmail.com</a>
            <h4 id="Resume">&nbsp;Resume:</h4>
            <hr>
            <a href="Aaron Pazdera Software Development Resume.pdf" target="_blank" rel="noopener noreferrer">pdf</a>
            <a href="Aaron Pazdera Software Development Resume.docx">docx</a>
        </div>
    </div>

    <!-- Add picture of self here -->
    <h1 id="Introduction">Introduction</h1>
    <p>My name is Aaron Pazdera, I'm an honors student at UW-Stout, and I spend a considerable amount of time
        programming. Often this is related to various hobbies, but I also have a tendency to take a dive over the deep
        end into the hardest problems I can
        find. What follows is a list of my projects. Feel free contact me to ask questions about them if you're
        interested, or for any other reason.</p>
    <br>


    <h1 id="Economic-Modeling">Economic Modeling Research</h1>
    <img class="model_graph"
        src="https://raw.githubusercontent.com/larissaford/PIC-Math-MSCS-390-001/master/FP_Graph.png"
        alt="Numerical methods used to solve a system of differential equations"><br>
    <a href="https://github.com/larissaford/PIC-Math-MSCS-390-001" target="_blank"
        rel="noopener noreferrer">https://github.com/larissaford/PIC-Math-MSCS-390-001</a><br><br>
    <p> Going into the summer, my internship prospects had been shattered by Covid. Many of my friends had their
        internships cancelled as well. Fortunately however, a deal was struck, research for internship requirement, and
        a team of six of us got to work on a project for University of Tampa professor John Stinespring.</p>
    <p> By coincidence, the way that we worked together ended up being very similar to a scrum team. We had a meeting at
        the beginning of the day where we talked about what we were doing, where we were at, and what needed to be done,
        updating our list of
        tasks. Then we split off to do our work and collaborated with each other, usually in groups of 2-3 on the
        theoretical and math intensive parts.</p>
    <p> In the end, we were successful at our task. Essentially, the task was to figure out a way to approximate the
        solution to a specific system of differential equations with only half the information that would be required
        for doing so algebraically.
        We figured out a good algorithm that converges on the right answer. We were in the middle of proving that it
        would always apply and always give the right answers for these types of problems no matter how the base
        equations were modified, but then
        the summer ended.</p>
    <p> Our solution ended up involving a series of simulations that get progressively more and more accurate, as
        pictured above. Each line is a lot like dropping a leaf into a river and seeing where it goes. The system of
        equations describes the river and
        how its flow changes over time, and we're trying to figure out where to put the leaf.</p>


    <h1 id="Perceptual-Image-Hashing">Perceptual Image Hashing</h1>
    <img src="Hannigan_aHash.png" alt="Image of Alyson Hannigan, resized to 8x8, then bits of hash are set">
    Images courtesy of:
    <a href="http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html" target="_blank"
        rel="noopener noreferrer">http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html</a><br><br>
    <a href="https://github.com/apaz-cli/Open-Image-Hashing-Tools" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/Open-Image-Hashing-Tools</a><br><br>
    <p> In the beginning, all that I wanted to do was write a script to remove near-duplicate images from a folder. How
        could that be hard? Turns out that it's actually a very hard problem. So then I took a dive off the deep end.
    </p>
    <p> In simplest terms, the task is to tell the computer how to "look at" an image and decide "what's important"
        about what it "looks like."
        Whatever that means. Usually in computer science, problems are strictly defined, and this one is definitely not.
        Naturally, there are lots of ways to go about it. Unfortunately, none of them are particularly great. The
        algorithms all have signigicant trade-offs in terms of speed, accuracy, and suitability.</p>
    <p> The search to the answer to this difficult and interesting problem has led me many places, from digital and
        analog image and signal processing, to computational geometry, to graph theory,
        and eventually to deep learning. All because I wanted to remove png and jpg duplicate images from a folder. </p>
    <p> I ended up doing much more than that. I built a perceptual image hashing framework for large scale distributed
        image processing pipelines.
        It can work across as many computers (or AWS instances) as necessary. My solution was more than a little bit
        overkill, but also whole lot of fun. </p>
    <p> Lastly, this project taught me a lot about system and library design. I built the entire system from scratch. I
        couldn&#39;t find a Java image library that could do pixel comparisons fast enough, so I built my own. I
        couldn't find a data structure to do large-scale k-nearest-neighbor queries efficiently enough, so I built my
        own. The framework has zero dependencies except for the Java standard library.</p>
    <br>

    <h1 id="MLPMixer">MLP-Mixer Torch</h1>
    </p><img src="MLP-Mixer.png"
        alt="A diagram detailing the construction of the MLP-Mixer deep neural network architecture for computer vision."><br>
    Image taken from Google's paper.<br>
    <a href="https://github.com/apaz-cli/Torch-MLP-Mixer" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/Torch-MLP-Mixer</a><br><br>

    <p>In this project, I implemented the paper <a href="https://arxiv.org/abs/2105.01601">"MLP-Mixer: An all-MLP
            Architecture for Vision"</a> by the Google Brain AI research team in PyTorch. The paper details how to build
        a machine learning model for computer vision, without the need for convolutional neural networks or vision
        transformer architectures.</p>


    <h1 id="Deep-Learning-Hash">Deep Learning Hash</h1>
    <img src="imgpair.png" alt="image next to the same image with added gaussian noise">
    <img src="loss.png" alt="Loss function mathematically described">
    <a href="https://github.com/apaz-cli/Torch" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/ML-ImageHash</a><br><br>
    <p> After a while, I began to tire of balancing the tradeoffs of different perceptual image hashing algorithms.
        Conventional image hashing algorithms are great and all, but I started to think that there&#39;s got to be
        a better way. I spent a long time thinking deeply about what was important to what an image &quot;looked
        like&quot; and the ways each algorithm did and didn&#39;t distill the &quot;essence,&quot; whatever that means,
        of an image. Eventually I realized that this would be a perfect application of machine learning.</p>

    <h4>Hard questions that I don't want to have to answer:</h4>
    <p>
    <ul>
        <li>How geometrically simple or complex are my images? Do the pixel depth maps look smooth or jagged?</li>
        <li>Do the images in my dataset compress well with a Discrete Cosine Transform?</li>
        <li>What do I need my algorithm to be invariant to? Noise? Translations? Rotations? How will that be
            accomplished?</li>
        <li>Do I have time to label a statistically significant portion of my dataset as duplicate or non-duplicate so
            that I can measure performance?</li>
        <li>Are my accuracy benchmarks good enough? Do I have accuracy benchmarks? If the answer is no, what should I
            do? If the answer is yes, how should my decision be informed?</li>
        <li>Given the number of labels and the size of the dataset, how statistically significant is this result?</li>
        <li>How much longer do I have before my research time is over and I need to have the answer to these questions?
        </li>
    </ul>
    </p>

    <p> Balancing which algorithm to use and justifying why is hard. That's why machine learning is so great here, I
        don't have to care. Instead of choosing the algorithm manually, I can simply describe the perfect algorithm for
        my dataset through constraint optimization. That way, the computer is the one who has to figure out the
        algorithm, and not me.</p>


    <h1 id="Safebooru-Scraper">Safebooru Scraper</h1>
    </p><img src="Safebooru.png" alt="Safebooru logo"><br>
    <a href="https://github.com/apaz-cli/Safebooru-Scraper" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/Safebooru-Scraper</a><br><br>
    <p> Deep learning takes a very large dataset to pull off. I needed a dataset. So I wrote a program that downloads
        roughly 6 million anime images from <a href="https://Safebooru.org/" target="_blank"
            rel="noopener noreferrer">Safebooru.org</a>. I now use this dataset to benchmark and train various
        perceptual image hashing algorithms.</p>
    <p> The next big issue that I found myself running into is download times. Even downloading the dataset at a rate of
        ten image thumbnails per second, it still took 3-4 days to download the whole thing. Once the download was
        complete, I was dismayed to learn that simply loading all the downscaled 64x64 images into memory from my
        external hard drive takes over
        sixteen hours. To train my deep learning model a single time, I have to do that somewhere between twenty to
        fifty times. So, a minimum of 13.3 days. SSD speeds are much faster, but I have limited storage. I'm currently
        in negotiations to get some
        better hardware and a video card to train on so that I can train a much larger and deeper network.</p>
    <br>


    <h1 id="VPTree">Vantage Point Tree</h1>
    </p><img src="vptree.png" alt="Vantage Point Tree construction diagram"><br>
    <a href="https://github.com/apaz-cli/VPTree" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/VPTree</a><br><br>
    <p> To check if two perceptual image hashes match, you simply compute the distance between them. If the distance
        between the two hashes falls within a certain threshold, then the images match. Otherwise, they don't.
        This is quite efficient, especially compared to manually staring at a screen, or compared to loading the images
        into memory. However, duplicate-finding means cross-comparing every image with every other image. I have 6
        million images. Six million squared is a very large number. If I were able to compare hashes at a rate of 100
        comparisons per second, it would take about 11415 years to compare them all. Clearly, that is not an option.</p>
    <p> The solution is to carefully store the hashes in a way that they can be easily partitioned. I chose a data
        structure called a Vantage Point Tree. The great thing about a vantage point tree is that it works on any
        concept of distance, not just standard Euclidean distance. This is useful because not all hashing algorithms use
        the same distance metric. Some use Euclidean distance, but some use Hamming distance, and this way I don't have
        to write multiple solutions.</p>
    <p> For technical reasons, I chose to write this algorithm from scratch in C. This is an incredibly
        performance-sensitive piece of code. If I had chosen to write it in Java or Python, or even naive
        C++, I would have run into some serious performance issues. But, I still needed to use this code in a Java
        program. So I wrote JNI language bindings to my C code so that I could interact with it inside the Java virtual
        machine. Doing this allowed me to reap the absurd performance benefits that come with well designed low level C
        code, while never leaving behind the ease of experimentation that a higher level language like Java provides.
    </p>
    <br>


    <h1 id="CProj">Other C Projects</h1>
    </p><img src="muxtexes.png" alt="Example code for cross-platform muxtexes."><br>
    <a href="https://github.com/apaz-cli/memdebug.h" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/memdebug.h</a><br><br>
    <a href="https://github.com/apaz-cli/threadpool.h" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/threadpool.h</a><br>
    <p> One night I decided: "You know what? I haven't slept in roughly 38 hours. Let's write a memory debugger from
        scratch." So then I did. My memory of that night is still a bit hazy due to the exhaustion, but over the course
        of a couple weeks afterwards
        I slowly refined it and added features. I made it thread-safe with the cross platform muxtexes above, and added
        a feature that lets you view every memory allocation that you've made and where, so it's easy to track down
        memory leaks.</p>
    <p> Later, I decided to write a threadpool from scratch also. In the future I plan to extend it to also implement
        futures.</p>
    <br>


    <h1 id="Brainfuck-Compiler-Interpreter">BF Compiler/Interpreter</h1>
    <pre><code>
    <span class="hljs-comment">This is Hello World in brainfuck:</span>
    <span class="hljs-literal">++++++++++[&gt;+++++++&gt;++++++++++&gt;+++&gt;+&lt;&lt;&lt;&lt;-]</span>
    <span class="hljs-literal">&gt;++.&gt;+.+++++++..+++.&gt;++.&lt;&lt;+++++++++++++++.</span>
    <span class="hljs-literal">&gt;.+++.------.--------.&gt;+.&gt;.</span>
    </code></pre>
    <a href="https://github.com/apaz-cli/Brainfuck-Tools" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/Brainfuck-Tools</a><br><br>

    <p> Brainfuck is an intentionally confusing programming language, aptly named for the experience of writing and
        debugging its code. It was not created to be useful, but to challenge, amuse, and punish programmers. Despite
        the mind bending complexity of BF programs, the language itself is quite simple.</p>
    <p> Building a compiler for a conventional programming language is very difficult. It's a relatively straightforward
        task in the same way as the twelve labours of Heracles. Writing an abstract syntax tree parser, a necessary part
        of most compilers, feels very much like trying to slay the Nemean lion.</p>
    <p> Fortunately though, writing a Brainfuck interpreter is relatively easy. So I did. Then I also wrote a compiler
        for it that transpiles Brainfuck to C code, then tells a C compiler to compile the generated code to produce a
        binary.</p>
    <p> Every programming language is in a (loose) sense just C in disguise, and BF is no exception. While you're
        writing C code, you start to think about how to design things like classes, inheritance, interfaces, lambda
        expressions, closures, futures, coroutines, and so on. It turns out these are all just C design patterns. I've
        definitely used each of those in C at least once. So, while compiling Brainfuck to C is somewhat trivial, the
        same logic can be applied to more complicated languages. After all, the first C++ compiler spat out C.</p>
    <br>


    <h1 id="RootwallaBot">RootwallaBot</h1>
    <img src="https://raw.githubusercontent.com/apaz-cli/RootwallaBot/master/Examples/RootwallaBot%20ProbChart%20Example.png"
        alt="Image of probability graph"><br>
    <a href="https://github.com/apaz-cli/RootwallaBot" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/RootwallaBot</a><br><br>
    <p> RootwallaBot is a Discord bot that does hypergeometric probability/distribution, and graphs the results.
        It&#39;s particularly useful to people like me who play a lot of trading card games such as Magic: the Gathering
        or Yugioh. </p>
    <p> While building a deck, you may think to yourself: If my 60 card deck contains 24 lands, what are the chances
        that I draw between 2 to 4 of them in my opening hand of 7 cards? You can see the graph of expected frequency
        above, along with mean and standard deviation. The command pictured above graphs relative frequency, but the
        /prob command will tell you that the probability is roughly 77.46%.</p>
    <br>


    <h1 id="MonikaBot">MonikaBot</h1>
    <img src="Monika_quote.png" alt="Monika Quote and image"><br>
    <a href="https://github.com/apaz-cli/MonikaBot" target="_blank"
        rel="noopener noreferrer">https://github.com/apaz-cli/MonikaBot</a><br><br>
    <p> MonikaBot is another Discord bot. This one just posts quotes and images of the character Monika from the game
        Doki Doki Literature Club. It also has some administrative features built into it.</p>
    <p> I&#39;m running both the Monika and Rootwalla bots from a Raspberry Pi in my dorm room. MonikaBot was created
        for a college club and gets some use from time to time. RootwallaBot was created for a Magic the Gathering
        Discord server and serves roughly 200 daily users.</p>
    <br>

    <br><br>
    <p>Also this website is a project too, I guess. If you want to call it that. Thanks for scrolling this far.</p>
    <p>--Aaron Pazdera</p>
</body>

</html>